{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initializing Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> f4912a2584d47a86bc18968c2d4419fd18490e2a
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> f4912a2584d47a86bc18968c2d4419fd18490e2a
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data... we skip the date format (column 1) because it doesn't work. \n",
    "# Wen Jun, do you have any idea to work it?\n",
    "X = np.loadtxt('Data/train_noatemp.csv',  delimiter=',', skiprows=1, usecols=range(2, 13))\n",
    "y = np.loadtxt('Data/train_noatemp.csv', delimiter=',', skiprows=1, usecols=[13])\n",
    "X_kaggle = np.loadtxt('Data/test_noatemp.csv', delimiter=',', skiprows=1, usecols=range(2,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, preprocessing, grid_search, cross_validation, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring / Model Evaluation in linear regression\n",
    "The `classifier.score(data, res)` in linear regression gives us the R-squared value of our model. It is a statistical measure of how close the data are to the fitted regression line. We get a min score of 0 and a max score of 1. Note that negative values are also possible, which means that we have made our prediction arbitrarily worse. There are two primary methods in `scikit` to score our model, `cross_val_score` and `cross_val_predict`. We can also find other indicators such as `mean absolute error`, `mean squared error` and `median absolute error`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `cross_val_score`\n",
    "As mentioned earlier, this gives us the `score` of the model. In this linear regression case, it gives us the R-squared value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.388648533452\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick sampling: Sample a training set while holding out 40% of the training data for testing our classifier\n",
    "We set random_state=0 so that it is seeded (always give us the same result)\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print(\"R2 Score: \" + str(regr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Accuracy: 0.21 (+/- 0.28)\n",
      "MAE Accuracy: -110.39 (+/- 53.85)\n",
      "MSE Accuracy: -21119.29 (+/- 20134.69)\n",
      "MedianAE Accuracy: -92.45 (+/- 49.13)\n"
     ]
    }
   ],
   "source": [
    "# Cross validation, folds = 5, KFolds (Fixed fold arrangements)\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "scores = cross_validation.cross_val_score(regr, X, y, cv=5)\n",
    "scores_MAE = cross_validation.cross_val_score(regr, X, y, cv=5, scoring = 'mean_absolute_error')\n",
    "scores_MSE = cross_validation.cross_val_score(regr, X, y, cv=5, scoring = 'mean_squared_error')\n",
    "scores_MedianAE = cross_validation.cross_val_score(regr, X, y, cv=5, scoring = 'median_absolute_error')\n",
    "\n",
    "# The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"R2 Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"MAE Accuracy: %0.2f (+/- %0.2f)\" % (scores_MAE.mean(), scores_MAE.std() * 2))\n",
    "print(\"MSE Accuracy: %0.2f (+/- %0.2f)\" % (scores_MSE.mean(), scores_MSE.std() * 2))\n",
    "print(\"MedianAE Accuracy: %0.2f (+/- %0.2f)\" % (scores_MedianAE.mean(), scores_MedianAE.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Accuracy: 0.39 (+/- 0.02)\n",
      "MAE Accuracy: -105.91 (+/- 2.48)\n",
      "MSE Accuracy: -19842.91 (+/- 990.60)\n",
      "MedianAE Accuracy: -84.20 (+/- 2.41)\n"
     ]
    }
   ],
   "source": [
    "# Cross validation, folds = 5, ShuffleSplit (Shuffled fold arrangements)\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(n=10886,n_iter=5, test_size=0.4, random_state=0)\n",
    "scores_shuffle = cross_validation.cross_val_score(regr, X, y, cv=cv)\n",
    "scores_shuffle_MAE = cross_validation.cross_val_score(regr, X, y, cv=cv, scoring = 'mean_absolute_error')\n",
    "scores_shuffle_MSE = cross_validation.cross_val_score(regr, X, y, cv=cv, scoring = 'mean_squared_error')\n",
    "scores_shuffle_MedianAE = cross_validation.cross_val_score(regr, X, y, cv=cv, scoring = 'median_absolute_error')\n",
    "\n",
    "# The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"R2 Accuracy: %0.2f (+/- %0.2f)\" % (scores_shuffle.mean(), scores_shuffle.std() * 2))\n",
    "print(\"MAE Accuracy: %0.2f (+/- %0.2f)\" % (scores_shuffle_MAE.mean(), scores_shuffle_MAE.std() * 2))\n",
    "print(\"MSE Accuracy: %0.2f (+/- %0.2f)\" % (scores_shuffle_MSE.mean(), scores_shuffle_MSE.std() * 2))\n",
    "print(\"MedianAE Accuracy: %0.2f (+/- %0.2f)\" % (scores_shuffle_MedianAE.mean(), scores_shuffle_MedianAE.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cross_val_predict`\n",
    "The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set (y_pred). Only cross-validation strategies that assign all elements to a test set exactly once can be used. Different scores may be produced because of the fold permutations. We also note that it only works for partitions (i.e. we can take indexes [1,2,3,4,5] but not [1,2,4,6,10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.366483960073\n",
      "MAE: 108.323674724\n",
      "MSE: 20785.8506192\n",
      "MedianAE: 87.2172045394\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Cross validation, folds = 5, KFolds (Fixed fold arrangements)\n",
    "regr = linear_model.LinearRegression()\n",
    "y_pred = np.abs(cross_validation.cross_val_predict(regr, X, y, cv=5))\n",
    "\n",
    "print(\"R2 Score: \" + str(metrics.r2_score(y,y_pred)))\n",
    "print(\"MAE: \" + str(metrics.mean_absolute_error(y,y_pred)))\n",
    "print(\"MSE: \" + str(metrics.mean_squared_error(y, y_pred)))\n",
    "print(\"MedianAE: \" + str(metrics.median_absolute_error(y,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results?\n",
    "Linear Regression does suck. R-squared values are only 40% optimal at best (60% deviation is huge) and this is obtained through *ShuffleSplit* cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with scaling\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "Scikit-learn offers tools to deal with this issue.\n",
    "\n",
    "This is something we learnt in class and we should implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.268361183313\n",
      "MAE: 112.88804777\n",
      "MSE: 24005.2882523\n",
      "MedianAE: 86.3235450494\n"
     ]
    }
   ],
   "source": [
    "def cross_validate_with_scaling(design_matrix, labels, classifier, cv_folds):\n",
    "    pred = np.zeros(labels.shape) # vector of 0 in which to store the predictions\n",
    "    for tr, te in cv_folds:\n",
    "        # Restrict data to train/test folds\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        #print Xtr.shape, ytr.shape, Xte.shape\n",
    "\n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "\n",
    "        # Fit classifier\n",
    "        classifier.fit(Xtr, ytr)\n",
    "\n",
    "        yte_pred = classifier.predict(Xte) # two-dimensional array\n",
    "        pred[te] = yte_pred                \n",
    "    return pred\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "shuffle_split_fold = cross_validation.ShuffleSplit(n=10886,n_iter=5, test_size=0.4, random_state=None)\n",
    "y_pred_scaled = cross_validate_with_scaling(X,y,regr,shuffle_split_fold)\n",
    "print(\"R2 Score: \" + str(metrics.r2_score(y,y_pred_scaled)))\n",
    "print(\"MAE: \" + str(metrics.mean_absolute_error(y,y_pred_scaled)))\n",
    "print(\"MSE: \" + str(metrics.mean_squared_error(y, y_pred_scaled)))\n",
    "print(\"MedianAE: \" + str(metrics.median_absolute_error(y,y_pred_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Application on kaggle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [-24 -28 -21 ..., 307 322 312]\n",
      "Y_pred: [ 24  28  21 ..., 307 322 312]\n"
     ]
    }
   ],
   "source": [
    "regr_kaggle = linear_model.LinearRegression()\n",
    "regr_kaggle.fit(X,y)\n",
    "pred_kaggle = regr_kaggle.predict(X_kaggle)\n",
    "print(\"Y_pred: \" + str(pred_kaggle.astype(int))) # we still need to abolute these values. \n",
    "print(\"Y_pred: \" + str(np.abs(pred_kaggle.astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [169 165 173 ..., 502 517 506]\n"
     ]
    }
   ],
   "source": [
    "regr_kaggle_scaled = linear_model.LinearRegression()\n",
    "\n",
    "# Scale our data\n",
    "scaler = preprocessing.StandardScaler(with_mean = True, with_std=False)\n",
    "# scaler.partial_fit(X)\n",
    "# X_scaled = scaler.transform(X)\n",
    "X_scaled = scaler.fit_transform(X,y)\n",
    "\n",
    "# Predict\n",
    "regr_kaggle_scaled.fit(X_scaled, y)\n",
    "pred_kaggle_scaled = regr_kaggle_scaled.predict(X_kaggle).astype(int)\n",
    "print(\"Y_pred: \" + str(pred_kaggle_scaled))\n",
    "#np.savetxt('linreg_scaled_X.csv', pred_kaggle_scaled, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [ -272   257   701 ..., 12135 12664 12931]\n"
     ]
    }
   ],
   "source": [
    "regr_kaggle_norm = linear_model.LinearRegression()\n",
    "\n",
    "# Normalize our data\n",
    "X_norm = preprocessing.normalize(X)\n",
    "\n",
    "# Predict\n",
    "regr_kaggle_norm.fit(X_norm, y)\n",
    "pred_kaggle_norm = regr_kaggle_norm.predict(X_kaggle).astype(int)\n",
    "print(\"Y_pred: \" + str(pred_kaggle_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled and Normed X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [ 179  158  247 ..., 3273 3366 3425]\n"
     ]
    }
   ],
   "source": [
    "regr_kaggle_final = linear_model.LinearRegression()\n",
    "\n",
    "# Scale our data\n",
    "scaler = preprocessing.StandardScaler(with_mean = True, with_std=False)\n",
    "# scaler.partial_fit(X)\n",
    "# X_scaled = scaler.transform(X)\n",
    "X_scaled = scaler.fit_transform(X,y)\n",
    "X_final = preprocessing.normalize(X_scaled)\n",
    "\n",
    "# Predict\n",
    "regr_kaggle_final.fit(X_final, y)\n",
    "pred_kaggle_final = regr_kaggle_final.predict(X_kaggle).astype(int)\n",
    "print(\"Y_pred: \" + str(pred_kaggle_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
